{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88af501b-729d-4596-9640-da470210f126",
   "metadata": {},
   "source": [
    "![Top <](./images/watsonxdata.png \"watsonxdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6556fca-c4eb-4e1a-a671-edc775339326",
   "metadata": {},
   "source": [
    "# Spark and watsonx.data Integration\n",
    "This notebook demonstrate how Spark can connect to watsonx.data and manipulate the data. This system has a local Spark engine that will be used to access watsonx.data. This is a minimally configured Spark engine, but is sufficient to demonstrate the steps needed to connect to watsonx.data and access the data that resides in the catalogs. Special thanks to Daniel Hancock on which this notebook was derived from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b543727c-7aeb-432f-a727-5bb52eefdbce",
   "metadata": {},
   "source": [
    "## Watsonx.data Development Systems Updates\n",
    "A number of configuration changes were made to the watsonx.data development system in order for these examples to run. \n",
    "* The MinIO server must have the 9000 port exposed in order to communicate with it. The default configuration for the watsonx.data server on TechZone runs in `diag` mode which automatically exposes this port. If you have not started the development server in `diag` mode, you can use the `ibm-lh-dev/bin/expose-minio` command to open up this port.\n",
    "* The Hive Metastore (lh-ibm-hive-metastore) container uses port 9083 to communicate with other programs. Unfortunately the 9083 port is not exposed in the container, so the container was modified to expose port 9083 and was restarted.\n",
    "* Ports 9000 and 9083 are exposed ports on the TechZone server so in theory you should be able to use external tools to access these ports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14fbd8a-e091-40ce-bda4-a75939ffa7a3",
   "metadata": {},
   "source": [
    "## Copy Spark Libraries\n",
    "The Spark libraries that are used by this notebook need to be loaded into the local file system in order for the spark calls to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e9eb4-dc34-4e92-ab53-bf346e1268fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%system tar -xf /spark/spark.tgz -C /usr/local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c914e9-b25f-4c73-bd67-8f88bdff8b78",
   "metadata": {},
   "source": [
    "## Environment Variables \n",
    "We need to make sure that a number of environment variables are set so that the Spark code can be accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16915d-8856-45f4-87c0-81d217e693ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env SPARK_HOME=/usr/local/spark\n",
    "%env PYSPARK_DRIVER_PYTHON=jupyter\n",
    "%env PYSPARK_DRIVER_PYTHON_OPTS=notebook\n",
    "%env PATH=/usr/local/bin:/usr/local/sbin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/spark/bin:/root/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f8dbc-1136-44d1-91df-cab3e4281c75",
   "metadata": {},
   "source": [
    "## System Variables\n",
    "In addition to the environment variables, we need to set some Python variables that will be used throughout the scripts. These settings are:\n",
    "* minio_host - The URL of the Minio server\n",
    "* minio_port - The port that the Minio server is using\n",
    "* hive_host  - The URL of the Hive server\n",
    "* hive_port  - The port that the Hive server is using\n",
    "\n",
    "Note that the URLs and PORTS are for an internal connection in the watsonx.data development server. These URLs and PORTS will be different if you are connecting externally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f1d359-31ac-4989-802f-60ea9af77f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_host    = \"watsonxdata\"\n",
    "minio_port    = \"9000\"\n",
    "hive_host     = \"watsonxdata\"\n",
    "hive_port     = \"9083\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05819345-1846-43cf-8b22-b9df80e2940c",
   "metadata": {},
   "source": [
    "# Load Demonstration Data\n",
    "The `staging-bucket` directory contains three files that will be used in the Spark examples:\n",
    "* customer.csv\n",
    "* orders.csv\n",
    "* products.csv\n",
    "\n",
    "Rather than using the MinIO UI to create a new bucket and upload these files, we will be using the MinIO CLI which provides direct access to the MinIO system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bfd3f2-989e-4685-b82f-ce8a8ad765a4",
   "metadata": {},
   "source": [
    "## Minio CLI\n",
    "In order to use the MinIO CLI, we must first register the MinIO server that we need to connect to. Before we do that we need to extract the passwords of the MinIO service, along with some other credentials. The passwords for all of the services can be found in the `/certs/passwords` file found in this server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c98174-fdc3-4f66-a701-43e39a56942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat /certs/passwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63114429-4ee3-4695-84bd-61f1de7162a6",
   "metadata": {},
   "source": [
    "The following code will extract all of the passwords and userids that are required for the MinIO and Spark connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd417f-c215-41ae-90f3-85458bf4f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hive_id           = None\n",
    "hive_password     = None\n",
    "minio_access_key  = None\n",
    "minio_secret_key  = None\n",
    "keystore_password = None \n",
    "cert_file         = \"/certs/lh-ssl-ts.jks\"\n",
    "\n",
    "try:\n",
    "    with open('/certs/passwords') as fd:\n",
    "        certs = fd.readlines()\n",
    "    for line in certs:\n",
    "        args = line.split()\n",
    "        if (len(args) >= 3):\n",
    "            system   = args[0].strip()\n",
    "            user     = args[1].strip()\n",
    "            password = args[2].strip()\n",
    "            if (system == \"Minio\"):\n",
    "                minio_access_key = user\n",
    "                minio_secret_key = password\n",
    "            elif (system == \"Thrift\"):\n",
    "                hive_id = user\n",
    "                hive_password = password\n",
    "            elif (system == \"Keystore\"):\n",
    "                keystore_password = password\n",
    "            else:\n",
    "                pass\n",
    "except Error as e:\n",
    "    print(\"Certificate file with passwords could not be found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb878343-0259-493f-87fb-b15789a0a18b",
   "metadata": {},
   "source": [
    "### Minio System Alias\n",
    "Before running any commands against the MinIO server, an alias needs to be created that includes the access and secret key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df42b03f-98d3-40be-9097-d53377ff5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%system mc alias set watsonxdata http://{minio_host}:{minio_port} {minio_access_key} {minio_secret_key}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80874e9-94f8-4921-9810-7b2e8cc79c38",
   "metadata": {},
   "source": [
    "### List Buckets\n",
    "The `mc` command provides us with a number of commands that allows us to manage buckets and files within them. The following command checks to see if the `staging-bucket` exists. This bucket is used for all of the Spark examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840e906-5aa0-41fa-919c-29ea0ec252b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%system mc ls tree watsonxdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f643c6-4fe1-45cd-8868-a28990178743",
   "metadata": {},
   "source": [
    "If the staging bucket exists, we will delete the bucket and the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e58e72c-6820-4231-a045-dbf6ceebfc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%system mc rb --force watsonxdata/staging-bucket "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e4244b-5457-4de9-b04a-f508d255a2a2",
   "metadata": {},
   "source": [
    "### Create a Bucket\n",
    "At this point we will create the staging bucket that we are doing to use to hold our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045fcdb5-c3c4-4570-a267-5e1c1283a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%system mc mb watsonxdata/staging-bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e77034d-83e5-4534-8c79-f3f4dfab8a07",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "Next we will load the data from the `/staging-bucket` directory. Note that we need to use the full name of the bucket. The `mc` command allows to select which files to place into a bucket, or an entire directory with recursion. In this case we are only going to select the csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae48e0a-c197-4693-ac1c-944c71494650",
   "metadata": {},
   "outputs": [],
   "source": [
    "%system mc cp /notebooks/staging-bucket/*.csv watsonxdata/staging-bucket/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5695f230-20c8-4bd3-b690-868d0af6926b",
   "metadata": {},
   "source": [
    "We can double check that our files are there with the `mc ls tree` command and using the `--files` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c664d0-837d-4cc6-8cdc-f25693eef53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%system mc tree --files watsonxdata/staging-bucket/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968439a2-e5b6-4a41-952b-a998c418cc4b",
   "metadata": {},
   "source": [
    "# Spark Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e436bd58-1b39-47ac-aa5b-a3749dc48d13",
   "metadata": {},
   "source": [
    "The next set of Python instructions will initialize the Spark connection. Once the connection is established to the engine, we need to update a number of values to provide credentials and a URL to the Hive and MinIO services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b709b62-daad-4999-8b2d-0654ff2bf5c6",
   "metadata": {},
   "source": [
    "### Initialize the Spark Connection\n",
    "Initialize the settings for the Spark service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9caa598-acfa-4fda-a9e2-1683ea40be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "spark = SparkSession.builder.appName('sparky').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "conf = sc.getConf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a036f-9866-4e82-8bb7-c3a967b1eee7",
   "metadata": {},
   "source": [
    "### Watsonx.data Configuration Information\n",
    "Once we have the configuration established, we need to update the values corresponding to our MinIO and Hive settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32a8c10-dc0a-4fca-a768-b016dd0eb60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = conf.set(\"spark.sql.debug.maxToStringFields\",                    \"100\")\n",
    "_ = conf.set(\"fs.s3a.path.style.access\",                             \"true\")\n",
    "_ = conf.set(\"fs.s3a.impl\",                                          \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "_ = conf.set(\"fs.s3a.connection.ssl.enabled\",                        \"true\")\n",
    "_ = conf.set(\"spark.driver.extraJavaOptions\",                        \"-Dcom.sun.jndi.ldap.object.disableEndpointIdentification=true\")\n",
    "\n",
    "_ = conf.set(\"spark.sql.catalogImplementation\",                      \"hive\")\n",
    "_ = conf.set(\"spark.sql.extensions\",                                 \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "_ = conf.set(\"spark.sql.iceberg.vectorization.enabled\",              \"false\")\n",
    "\n",
    "_ = conf.set(\"spark.sql.defaultCatalog\",                             \"iceberg_data\")\n",
    "_ = conf.set(\"spark.sql.catalog.iceberg_data\",                       \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "_ = conf.set(\"spark.sql.catalog.iceberg_data.type\",                  \"hive\")\n",
    "_ = conf.set(\"spark.sql.catalog.iceberg_data.uri\",                   f\"thrift://{hive_host}:{hive_port}\")\n",
    "\n",
    "_ = conf.set(\"spark.hive.metastore.client.auth.mode\",                \"PLAIN\")\n",
    "_ = conf.set(\"spark.hive.metastore.client.plain.username\",           hive_id)\n",
    "_ = conf.set(\"spark.hive.metastore.client.plain.password\",           hive_password)\n",
    "\n",
    "_ = conf.set(\"spark.hive.metastore.use.SSL\",                         \"true\")\n",
    "_ = conf.set(\"spark.hive.metastore.truststore.type\",                 \"jks\")\n",
    "_ = conf.set(\"spark.hive.metastore.truststore.path\",                 cert_file)\n",
    "_ = conf.set(\"spark.hive.metastore.truststore.password\",             keystore_password)\n",
    "_ = conf.set(\"spark.hive.metastore.uris\",                            f\"thrift://{hive_host}:{hive_port}\")\n",
    "\n",
    "_ = conf.set(\"spark.hadoop.fs.s3a.endpoint\",                         f\"http://{minio_host}:{minio_port}\")\n",
    "_ = conf.set(\"spark.hadoop.fs.s3a.access.key\",                       minio_access_key)\n",
    "_ = conf.set(\"spark.hadoop.fs.s3a.secret.key\",                       minio_secret_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3ef828-0042-45d2-b2f8-6679ab7524ac",
   "metadata": {},
   "source": [
    "### Restart Spark with new Configuration\n",
    "To make the configuration changes take effect, we need to stop the Spark services and recreate it with the new configuration information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b6685-60a1-466e-b9be-8606e49d4574",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "conf = sc.getConf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb4092b-31f6-4fee-94f6-c00888024acd",
   "metadata": {},
   "source": [
    "If you want to review the settings, run the following code to get the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb98080-4c1f-4998-8788-7f777746d5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sc.getConf().getAll()\n",
    "for i in c:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3598950-b220-49db-9a4e-cbe76e25d64a",
   "metadata": {},
   "source": [
    "### Spark SQL Helper Code\n",
    "The follow Python will execute Spark SQL and return the success or error status from the call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e4c98b-b61d-49fd-9e4b-c4e9bd48d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparksql - Run SQL statement and display results if it is a DML statement\n",
    "# sqltext     -> valid SQL statement\n",
    "\n",
    "def sparksql(sqltext):\n",
    "\n",
    "    if (sqltext in [None,\"\"]):\n",
    "        print(\"Invalid SQL command\")\n",
    "        return\n",
    "        \n",
    "    print(f\"SQL: {sqltext}\")\n",
    "    keywords = sqltext.split()\n",
    "    sqltype  = keywords[0].upper()\n",
    "\n",
    "    try:\n",
    "        if (sqltype in [\"DROP\",\"CREATE\",\"INSERT\",\"DELETE\",\"UPDATE\",\"ALTER\"]):\n",
    "            spark.sql(sqltext)  # use for sql statements that don't have results (create, drop, use, etc.) or to omit results output\n",
    "            print(\"The SQL command completed successfully.\\n\") \n",
    "        else:\n",
    "            spark.sql(sqltext).show() # show the results in table format\n",
    "           \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98285e07-c973-4682-bcbf-e23084579398",
   "metadata": {},
   "source": [
    "### Set variables used in notebook\n",
    "The following variables will be used throughout the scripts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14036707-8867-46d8-872d-bf0f6ef2c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog        = \"iceberg_data\"\n",
    "schemas        = ['bronze', 'silver', 'gold'] # don't change order of schemas in list\n",
    "tables         = ['customers', 'products', 'orders']   # don't change order of tables in list, customer must be first\n",
    "summ_tables    = ['customer_activity_summary', 'order_summary', 'product_category_summary']\n",
    "iceberg_bucket = \"iceberg-bucket\"\n",
    "staging_bucket = \"staging-bucket\"\n",
    "files = {\n",
    "    'customers': 'customer.csv',\n",
    "    'products': 'products.csv',\n",
    "    'orders': 'orders.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863964eb-ef62-42a6-b754-11fc89ad6775",
   "metadata": {},
   "source": [
    "### Example Reset - Drop Schemas and Tables\n",
    "If you are running this notebook again, there will be files and schemas that exist in the system which will cause subsequent scripts to fail. Run this cell to make sure that any objects are deleted from the system. Note that you might get an error if an object does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71b573b-6911-48b9-a4f5-cbee90115a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show tables in each schema\n",
    "for schema in schemas:\n",
    "    sparksql(\n",
    "        f\"SHOW TABLES from {schema}\"\n",
    "    )\n",
    "  \n",
    "# drop tables\n",
    "for schema in schemas:\n",
    "    for table in tables:\n",
    "        sparksql(\n",
    "            f\"drop table if exists {schema}.{table}\"\n",
    "        )\n",
    "\n",
    "for table in summ_tables:\n",
    "    sparksql(\n",
    "        f\"drop table if exists {schema}.{table}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# drop schemas\n",
    "sparksql(\n",
    "    \"SHOW SCHEMAS IN iceberg_data\"\n",
    ")\n",
    "\n",
    "for schema in schemas:\n",
    "    sparksql(\n",
    "        f\"DROP SCHEMA IF EXISTS {schema}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f1c56d-10ac-447e-9cd1-b754002fff1a",
   "metadata": {},
   "source": [
    "### Check Iceberg Catalog\n",
    "We can use Spark to examine the contents of a catalog by connecting to the Hive service. The following statement will show the schemas in the iceberg catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad660723-7713-4289-b1aa-a3ca9a62e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"SHOW SCHEMAS FROM {catalog}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b126fd07-42cd-413a-a1f2-70a0b3bee239",
   "metadata": {},
   "source": [
    "# Data Organization in a Lakehouse\n",
    "A typical data lakehouse can classify data into different tiers based on the state of the data - raw, filtered, and optimized. Literature sometimes refers to these levels as Bronze, Silver, and Gold based on how well the data is refined.\n",
    "\n",
    "In this notebook, we are going to use the Bronze, Silver and Gold classification:\n",
    "* Bronze - This is the raw data that is kept in its original form. Examples of this type of data includes CSVs (Comma Separated Values), PDFs, images, JSON, and Text documents. There has been no filtering or refinement done on these data sets. Bronze data typically sits in an object store to minimize storage costs.\n",
    "* Silver - Silver data refers to Bronze objects which have been refined to produce a queryable object (table) that has had some filtering and cleansing applied to it. Silver data provides good query performance and the tables that are created can be stored in object store or in higher performance file systems.\n",
    "* Gold - Gold data further refines the Silver data. Data may be combined from various sources to create a single object to remove the need for joins. The data will be filtered to remove any inconsistent data. Gold data can reside on object stores but usually for performance reasons it will reside on high-performance storage using proprietary database engines to optimize query performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c755786c-7fe1-4e1b-9e89-0c9442c0b64b",
   "metadata": {},
   "source": [
    "## Bronze Data Layer\n",
    "The Bronze data layer will contain the raw data (CSV) files that be refined in subsequent steps. This first command will create the `bronze` schema for loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b03b4d3-2975-4855-967b-bd2785abba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schemas[0]} LOCATION 's3a://{iceberg_bucket}/{schemas[0]}'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f399681-a89f-4c89-93f2-1cddc138d94f",
   "metadata": {},
   "source": [
    "Check that the schema was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249fd21-2099-48d1-ab3f-38ef09e8d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"SHOW SCHEMAS FROM {catalog}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3657e54-c3c4-4810-9438-da14814ffd5b",
   "metadata": {},
   "source": [
    "### Ingest and validate tables\n",
    "This SQL will read the CSV from Object Storage and do the following:\n",
    "1. Infer schema from csv file\n",
    "2. Show schema from csv file\n",
    "3. Dislay the first 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c997a-2c2d-4a06-88ce-48677d597f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files.values():\n",
    "    print(f\"File: {staging_bucket}/{file}\")\n",
    "    try:\n",
    "        df_customer = spark.read \\\n",
    "        .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "        .option('header', True)\\\n",
    "        .option(\"inferSchema\", True)\\\n",
    "        .option(\"samplingRatio\", 0.25)\\\n",
    "        .csv(f\"s3a://{staging_bucket}/{file}\")\n",
    "\n",
    "        df_customer.printSchema()\n",
    "        df_customer.show(3)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc07384-0004-417b-83bd-4abd27f97221",
   "metadata": {},
   "source": [
    "### Show iceberg tables\n",
    "Doublecheck that there are no tables in the `bronze` schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609499e-3d09-495d-a4d0-77a364225003",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"SHOW TABLES from {catalog}.{schemas[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f00f625-f9b2-4495-bd8f-e806649158b3",
   "metadata": {},
   "source": [
    "### Ingest customer table\n",
    "This code will read the data (see the above example) and then immediately write it out to the `bronze` schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c832b-f41f-411d-a559-570e74186f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = files['customers']\n",
    "print(f\"Ingesting customer table using spark\")\n",
    "print(f\"File: {staging_bucket}/{file}\")\n",
    "   \n",
    "try:\n",
    "    df_customer = spark.read \\\n",
    "    .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat') \\\n",
    "    .option('header', True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .option(\"samplingRatio\", 0.25) \\\n",
    "    .csv(f\"s3a://{staging_bucket}/{file}\")   \n",
    "        \n",
    "    df_customer.printSchema()\n",
    "    df_customer.show(3)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "try:\n",
    "    df_customer.writeTo(f\"{catalog}.{schemas[0]}.{tables[0]}\") \\\n",
    "        .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "        .createOrReplace()\n",
    "    print(\"The INGEST command completed successfully.\\n\\n\")  \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f7160e-5094-4a58-89e4-d8a5e4936169",
   "metadata": {},
   "source": [
    "We check to make sure the customer file has been loaded into the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94384e9a-1ecd-406d-9594-54dfc7b75e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer table expected\n",
    "sparksql(\n",
    "    f\"SHOW TABLES from {schemas[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fbac44-968f-41de-9d44-c0bc42030ee0",
   "metadata": {},
   "source": [
    "### Describe the Customer table\n",
    "We can use the DESCRIBE function to print the column names and types for the customer table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb46be80-2bd2-4482-9ed3-1e403a07a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"DESCRIBE {schemas[0]}.{tables[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8796414d-566c-45da-944b-5d0a804fcd03",
   "metadata": {},
   "source": [
    "### Display Data\n",
    "We can check to see whether or not the data is in the customer table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d78e3-4bcf-47a3-bfe2-7de817380496",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql( \n",
    "    f\"SELECT * FROM {schemas[0]}.{tables[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f294c0-3cd8-4e73-9cdb-1082ce9b1156",
   "metadata": {},
   "source": [
    "Just to doublecheck, we make sure that the row count for the customer table is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7991a0-c80a-4e8f-82d6-5bb84ce9b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"SELECT count(*) FROM {schemas[0]}.{tables[0]}\"\n",
    ")\n",
    "print(\"Table customers expected rowscount: 102\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558b4554-47f2-4961-b4bd-43245b451ab4",
   "metadata": {},
   "source": [
    "## Ingest Product and Orders Tables\n",
    "We now ingest the other two tables, products and orders, into the `bronze` schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a612b88f-5b6f-46af-81e8-f882df818653",
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in tables[1:]: # skip customers table (ingested in previous step)\n",
    "    print(f\"Ingesting {table} table using spark\")\n",
    "    print(f\"File: {staging_bucket}/{file}\")    \n",
    "    file = files[f'{table}']\n",
    "    df_table = spark.read \\\n",
    "    .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat') \\\n",
    "    .option('header', True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .option(\"samplingRatio\", 0.25) \\\n",
    "    .csv(f\"s3a://{staging_bucket}/{file}\")\n",
    "    \n",
    "    try:\n",
    "        df_table.writeTo(f\"{catalog}.{schemas[0]}.{table}\") \\\n",
    "            .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "            .createOrReplace()\n",
    "        print(\"The INGEST command completed successfully.\\n\\n\")  \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c83d6f-08f0-40a2-8560-31910e17a354",
   "metadata": {},
   "source": [
    "### Bronze schema tables\n",
    "All three tables should now reside in the `Bronze` schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350616e-b738-4e27-b398-3df22a3ef401",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"SHOW TABLES from {catalog}.bronze\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b3dff9-fbc2-40b4-8614-7017daaaf31a",
   "metadata": {},
   "source": [
    "Check the row count for the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b94512",
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in tables:\n",
    "    print(f\"Table: {table}\")\n",
    "    sparksql(\n",
    "        f\"SELECT count(*) from bronze.{table}\"\n",
    "    )\n",
    "    \n",
    "print(\"customers expected rows: 102\")\n",
    "print(\"products  expected rows: 22\")\n",
    "print(\"orders    expected rows: 501\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f6ee9-5c3a-4cd4-a4d2-1db70ea13312",
   "metadata": {},
   "source": [
    "# Silver Data Layer\n",
    "Silver data contains objects which have been refined to produce a queryable object (table) that has had some filtering and cleansing applied to it. Silver data provides good query performance and the tables that are created can be stored in object store or in higher performance file systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c85a3-c7ee-427e-8a1e-1c354b679ef2",
   "metadata": {},
   "source": [
    "## Add Silver Schema\n",
    "We start by creating the `silver` schema that will be used for the refined tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39cc517-f3d3-4601-ad36-c8d616cb20af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"CREATE SCHEMA IF NOT EXISTS {catalog}.silver LOCATION 's3a://{iceberg_bucket}/{schemas[1]}'\"\n",
    ")\n",
    "\n",
    "# show the schemas in the iceberg catalog\n",
    "sparksql(\n",
    "    f\"SHOW SCHEMAS FROM {catalog}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3a2336-5034-4eed-9924-2bccc256426a",
   "metadata": {},
   "source": [
    "## Create and Cleanse the Customer table\n",
    "The first step to cleansing the customer data is to check for fields that are invalid or empty. In the case of the customer table, we are going to remove customers who have an invalid `customer_id` or `customer_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cee391-af5b-48a6-b980-c42e387f2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"SELECT count(*) from {catalog}.{schemas[0]}.{tables[0]}\"\n",
    ")\n",
    "\n",
    "sparksql(\n",
    "    f'''\n",
    "    CREATE OR REPLACE TABLE {catalog}.silver.{tables[0]} AS\n",
    "    SELECT\n",
    "      customer_id,\n",
    "      customer_name,\n",
    "      email,\n",
    "      phone_number\n",
    "    FROM\n",
    "      {catalog}.bronze.{tables[0]}\n",
    "    WHERE\n",
    "      customer_id IS NOT NULL\n",
    "      AND customer_name IS NOT NULL\n",
    "      ;\n",
    "    '''\n",
    ")\n",
    "\n",
    "sparksql(\n",
    "    f\"SELECT count(*) from {catalog}.silver.{tables[0]}\"\n",
    ")\n",
    "    \n",
    "print(f\"{catalog}.silver.{tables[0]} expected rows inserted: 100\")\n",
    "print(f\"{catalog}.silver.{tables[0]} expected rows cleansed:   2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f374f60-6acc-44d0-bb69-b58da3d0e7d1",
   "metadata": {},
   "source": [
    "## Create and Cleanse the Product table\n",
    "The product table needs to be checked for invalid product_id, category, or price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eaada3-1edd-43cf-95fb-d2610b585573",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"SELECT count(*) from {catalog}.bronze.{tables[1]}\"\n",
    ")\n",
    "\n",
    "sparksql(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {catalog}.silver.{tables[1]}  AS\n",
    "    SELECT\n",
    "      product_id,\n",
    "      product_name,\n",
    "      category,\n",
    "      price\n",
    "    FROM\n",
    "      {catalog}.bronze.{tables[1]} \n",
    "    WHERE\n",
    "      product_id IS NOT NULL\n",
    "      AND product_name IS NOT NULL\n",
    "      AND category IS NOT NULL\n",
    "      AND price >= 0\n",
    "      ;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "sparksql(\n",
    "    f\"SELECT count(*) from {catalog}.silver.{tables[1]}\"\n",
    ")\n",
    "    \n",
    "print(f\"{catalog}.{schemas[1]}.{tables[1]} expected rows inserted:  20\")\n",
    "print(f\"{catalog}.{schemas[1]}.{tables[1]} expected rows cleansed:   2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee92a874-1f7b-4311-a06c-e615611cdd54",
   "metadata": {},
   "source": [
    "## Create and Cleanse the Orders table\n",
    "The final orders table needs to check 5 columns to make sure they are valid:\n",
    "* order_id\n",
    "* order_date\n",
    "* customer_id\n",
    "* product_id\n",
    "* quantity\n",
    "* unit_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7989d59f-33a1-42f1-a8dd-d2f4e70233ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"SELECT count(*) from {catalog}.bronze.{tables[2]}\"\n",
    ")\n",
    "\n",
    "sparksql(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {catalog}.silver.{tables[2]}  AS\n",
    "    SELECT\n",
    "      order_id,\n",
    "      order_date,\n",
    "      customer_id,\n",
    "      product_id,\n",
    "      quantity,\n",
    "      unit_price,\n",
    "      unit_price * quantity as total_price -- add a total price column\n",
    "    FROM\n",
    "      {catalog}.bronze.{tables[2]}\n",
    "    WHERE\n",
    "      order_id IS NOT NULL\n",
    "      OR order_date IS NOT NULL\n",
    "      OR customer_id IS NOT NULL\n",
    "      OR product_id IS NOT NULL\n",
    "      OR quantity > 0\n",
    "      OR unit_price >= 0\n",
    "      ;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "sparksql(\n",
    "    f\"SELECT count(*) from {catalog}.silver.{tables[2]}\"\n",
    ")\n",
    "\n",
    "print(f\"{catalog}.silver.{tables[2]} expected rows inserted: 500\")\n",
    "print(f\"{catalog}.silver.{tables[2]} expected rows cleansed:   1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d5cead-b77c-4d70-a4c6-797c4f5890d0",
   "metadata": {},
   "source": [
    "# Gold Data Layer\n",
    "The Gold data further refines the Silver data and can be considered \"business-level\" because it is considered a trusted source of information. Data may be combined from various sources to create a single object to remove the need for joins. Gold data can reside on object stores but usually for performance reasons it will reside on high-performance storage using proprietary database engines to optimize query performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c549d-7b8e-411c-b804-ab36b044bee3",
   "metadata": {},
   "source": [
    "## Add Gold Schema\n",
    "We start by creating the `gold` schema that will be used for the refined Silver tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40971e00-1af3-4841-b0f8-b2805a7e0811",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"CREATE SCHEMA IF NOT EXISTS {catalog}.gold LOCATION 's3a://{iceberg_bucket}/{schemas[2]}'\"\n",
    ")\n",
    "\n",
    "# show the schemas in the iceberg catalog\n",
    "sparksql(\n",
    "    f\"SHOW SCHEMAS FROM {catalog}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95457f1b-4147-42bc-94d7-4a3b8b4b79ba",
   "metadata": {},
   "source": [
    "## Summary Tables\n",
    "In order to improve query performance, a series of summary tables are created on each of the base tables found in the `silver` schema. These tables provide summary information on:\n",
    "* Customer Activity\n",
    "* Product Orders\n",
    "* Product Catagories\n",
    "### Create Customer Activity Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286bc150-1065-4c25-a791-539c224f92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {catalog}.gold.customer_activity_summary  AS\n",
    "    SELECT\n",
    "        o.customer_id,\n",
    "        COUNT(DISTINCT o.order_id) AS total_orders_placed,\n",
    "        SUM(o.total_price) AS total_revenue\n",
    "    FROM\n",
    "        {catalog}.silver.{tables[2]} o\n",
    "    JOIN\n",
    "         {catalog}.silver.{tables[0]} c ON o.customer_id = c.customer_id\n",
    "    GROUP BY\n",
    "        o.customer_id;\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7dfad-2af9-4191-bb03-87ba61fe91c5",
   "metadata": {},
   "source": [
    "### Create Order Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2fc121-72f5-48a8-b8ae-4e908911eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {catalog}.gold.order_summary AS\n",
    "    SELECT\n",
    "        product_id,\n",
    "        SUM(quantity) AS total_quantity,\n",
    "        SUM(total_price) AS total_revenue\n",
    "    FROM\n",
    "        {catalog}.silver.{tables[2]}\n",
    "    GROUP BY\n",
    "        product_id;\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e6f7a-5196-4f5d-8e8f-8c611c9520e0",
   "metadata": {},
   "source": [
    "### Create Product Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac76e23-4fde-4c75-a8b9-cada15dc5bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE {catalog}.gold.product_category_summary AS\n",
    "    SELECT\n",
    "        category,\n",
    "        COUNT(*) AS product_count\n",
    "    FROM\n",
    "        {catalog}.silver.{tables[1]}\n",
    "    GROUP BY\n",
    "        category;\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dc95f5-5e9e-41da-b3dd-dad9cc82d268",
   "metadata": {},
   "source": [
    "## Sample Reports\n",
    "The following examples show the use of `silver` and `gold` data being joined in queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161964e-35a7-4958-a7ba-d5be881ec06b",
   "metadata": {},
   "source": [
    "### Report on Customer Activity: SILVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91edcb2b-941c-4636-b1bc-cd71e2e1c351",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "        c.customer_id,\n",
    "        c.customer_name,\n",
    "        COUNT(DISTINCT o.order_id) AS total_orders_placed,\n",
    "        SUM(o.total_price) AS total_revenue\n",
    "    FROM\n",
    "         {catalog}.silver.customers c\n",
    "    LEFT JOIN\n",
    "         {catalog}.silver.orders o ON c.customer_id = o.customer_id\n",
    "    GROUP BY\n",
    "        c.customer_id, c.customer_name\n",
    "    ORDER BY\n",
    "        total_revenue DESC;\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49e1450-bfc5-4234-8393-75f207a6fc34",
   "metadata": {},
   "source": [
    "### Report Total Revenue by Product Category: SILVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c677f7-7f3d-4f29-93c4-5f7d860f934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "        p.category,\n",
    "        SUM(o.total_price) AS total_revenue\n",
    "    FROM\n",
    "        {catalog}.silver.orders o\n",
    "    JOIN\n",
    "        {catalog}.silver.products p ON o.product_id = p.product_id\n",
    "    GROUP BY\n",
    "        p.category\n",
    "    ORDER BY\n",
    "        total_revenue DESC;\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f394f158-0c12-4b34-81bd-c6637c0e7e7d",
   "metadata": {},
   "source": [
    "### Report on Product Sales Summary: SILVER and GOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa140e-5ba3-4f7c-b6af-2e972a89fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "        p_silver.product_id,\n",
    "        p_silver.product_name,\n",
    "        p_gold.total_quantity,\n",
    "        p_gold.total_revenue\n",
    "    FROM\n",
    "        {catalog}.silver.products p_silver\n",
    "    JOIN\n",
    "        {catalog}.gold.order_summary p_gold ON p_silver.product_id = p_gold.product_id\n",
    "    ORDER BY\n",
    "        p_gold.total_revenue DESC;\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de914c8b-93f7-47cd-8b2f-4833d7596a70",
   "metadata": {},
   "source": [
    "## Spark SQL with Insert/Update/Delete\n",
    "We can use Spark SQL to insert, update, and delete records in the database. This first SQL statement will find all of the customers who have no CUSTOMER_ID or the CUSTOMER_NAME is NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e804215-86ca-4efe-bd15-9f9a6d41b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM\n",
    "        {catalog}.bronze.customers\n",
    "    WHERE\n",
    "        customer_id IS NULL OR\n",
    "        customer_name IS NULL\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2050db19-ad69-47be-ad72-ca331fe37df6",
   "metadata": {},
   "source": [
    "### Delete a Record\n",
    "The following SQL will delete the customer that does not have a CUSTOMER_ID or has no CUSTOMER_NAME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41abe22e-2649-459b-be55-6f7a2b976c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"\"\"\n",
    "    DELETE \n",
    "    FROM\n",
    "        {catalog}.bronze.customers\n",
    "    WHERE\n",
    "        customer_id IS NULL OR customer_name IS NULL\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d65367a-77bb-416b-acff-61b46d225c36",
   "metadata": {},
   "source": [
    "We double check to make sure the records have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2db9c1-df64-4340-9161-84fdd9dd545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM\n",
    "        {catalog}.bronze.customers\n",
    "    WHERE\n",
    "        customer_id IS NULL OR customer_name IS NULL\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb938e6-8e2f-41c6-bd0b-bb06a1121b69",
   "metadata": {},
   "source": [
    "### Insert a New Record\n",
    "We can insert a new record into a table. The next SQL statement will add a new customer to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a45fc5c-79d6-46c6-b031-5a54df54258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"\"\"\n",
    "    INSERT INTO {catalog}.bronze.customers\n",
    "    VALUES (\n",
    "       99999,\n",
    "       'A New Customer',\n",
    "       '209 Somewhere Street',\n",
    "       'newcustomer@ymail.com',\n",
    "       '800-555-1212'\n",
    "    )\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f992d0f2-926b-4310-8400-9015922fc26e",
   "metadata": {},
   "source": [
    "Check to see if our new customer exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1369eee-94e9-4a7f-9089-901a92191747",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM\n",
    "        {catalog}.bronze.customers\n",
    "    WHERE\n",
    "        customer_id = 99999\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4433990-b914-44de-8a2c-feb84f870abf",
   "metadata": {},
   "source": [
    "### Update a Record\n",
    "We can also update a record. The next statement will update the customer phone_number to a new value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd46e6-0079-4f19-8e64-31bc569e7139",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"\"\"\n",
    "    UPDATE {catalog}.bronze.customers\n",
    "    SET \n",
    "       phone_number = '888-333-5555'\n",
    "    WHERE\n",
    "       customer_id = 99999\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1345327-4db5-4d91-93e0-55a563e9161a",
   "metadata": {},
   "source": [
    "Check to see if our customer has a new phone number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82414a6-cb13-40c8-b9ab-edb4e8511cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksql(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM\n",
    "        {catalog}.bronze.customers\n",
    "    WHERE\n",
    "        customer_id = 99999\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cfe459",
   "metadata": {},
   "source": [
    "## Reset Examples\n",
    "Remove all tables and schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f755575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show tables in each schema\n",
    "for schema in schemas:\n",
    "    sparksql(\n",
    "        f\"SHOW TABLES from {schema}\"\n",
    "    )\n",
    "  \n",
    "# drop tables\n",
    "for schema in schemas:\n",
    "    for table in tables:\n",
    "        sparksql(\n",
    "            f\"drop table if exists {schema}.{table}\"\n",
    "        )\n",
    "\n",
    "for table in summ_tables:\n",
    "    sparksql(\n",
    "        f\"drop table if exists {schema}.{table}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# drop schemas\n",
    "sparksql(\n",
    "    \"SHOW SCHEMAS IN iceberg_data\"\n",
    ")\n",
    "\n",
    "for schema in schemas:\n",
    "    sparksql(\n",
    "        f\"DROP SCHEMA IF EXISTS {schema}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc56c0",
   "metadata": {},
   "source": [
    "Remove the bucket from MinIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2214ec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%system mc rb --force watsonxdata/staging-bucket "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d1caf-5959-4cfe-9455-f2058f9bac4f",
   "metadata": {},
   "source": [
    "#### Credits: IBM 2024, George Baklarz [baklarz@ca.ibm.com], Daniel Hancock [daniel.hancock@us.ibm.com]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
