{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03add16d-8c31-4603-bba1-6a1ab8703bf9",
   "metadata": {},
   "source": [
    "![Top <](./images/watsonxdata.png \"watsonxdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbc29d5-76b4-4c9c-abdd-eaf8b4c00ebf",
   "metadata": {},
   "source": [
    "# Watsonx and Milvus\n",
    "\n",
    "Watsonx is IBM's platform committed to injecting generative AI into services that span across customer's data lifecycle. Each of the services offer a unique experience but when combined together, the business value is even stronger. This demonstration features:\n",
    "\n",
    "  1. Scraping data from Wikipedia and other web articles into a Jupyter Notebook\n",
    "  2. Inserting web data into watsonx.data\n",
    "  3. Vectorizing data in watsonx.data and inserting it into the Milvus vector database\n",
    "  4. Retrieve prompts from Milvus that can be embedded into a Large Language Model \n",
    "\n",
    "#### Credits\n",
    "\n",
    "This material has been adopted from material originally produced by Katherine Ciaravalli and Ken Bailey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d38efd-f29a-439d-b2f4-86d185bc3952",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG)\n",
    "Retrieval Augmented Generation is an AI framework for retrieving facts from an external knowledge base to ground large language models (LLMs) on the most accurate, up-to-date information and to give users insight into LLMs' generative process. This can improve the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information. Implementing RAG in an LLM-based question answering system has two main benefits: It ensures that the model has access to the most current, reliable facts, and that users have access to the model’s sources, ensuring that its claims can be checked for accuracy and ultimately trusted.\n",
    "\n",
    "In this example we will use Wikipedia articles on a specific topic, Climate Change. We want to explore answering business questions related to this topic. As the environment continues to change, businesses will need to take into consideration how these changes will impact their operations. Combining additional climate change data alongside business specific data would allow companies to prose meaningful questions and consider alternative outcomes when determining effective business strategies. Although Wikipedia is not the most trusted source, this is an introductory demo, intended to highlight the ease of use of incorporating new information into Large Language Models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8dec3",
   "metadata": {},
   "source": [
    "## Load Wikipedia Data\n",
    "\n",
    "This notebook walks through the process of loading a wikipedia article into a watsonx.data relational database table. We use the [Wikipedia python library](https://pypi.org/project/wikipedia/) to retrieve wikipedia articles. We then create a table in the database to store the articles. Finally, we load the articles into the database. \n",
    "\n",
    "For details on the copyright issues when extracting data, please refer to the [Wikipedia Copyrights](https://en.wikipedia.org/wiki/Wikipedia:Copyrights) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae50a6",
   "metadata": {},
   "source": [
    "### Install required libraries\n",
    "\n",
    "A couple of additional libraries need to be loaded into the notebook to order to query Wikipedia articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb1010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install wikipedia\n",
    "!pip install pymilvus\n",
    "!pip install sentence_transformers\n",
    "!pip install grpcio==1.60.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d080c362",
   "metadata": {},
   "source": [
    "### Fetch Wikipedia Articles\n",
    "\n",
    "The following code will search Wikipedia articles and display a list of the articles by title. The initial search will return a list of up to 10 titles, while the subsequent call will retrieve the summary of the article. The two results are combined into one dataframe for easy scrolling.\n",
    "\n",
    "Update the next field to include what you are searching for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cdc0fd-a213-485a-bf7f-8e7f5660bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"climate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a03287-ba9a-4961-a8e9-2c906c565fb0",
   "metadata": {},
   "source": [
    "### Retrieve 10 Articles\n",
    "The next call will retrieve a maximum of 10 titles and display the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc02a33-de56-4b10-85cb-5f76971a202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "search_results = wikipedia.search(topic)\n",
    "print(\"Article Title\")\n",
    "print(\"-------------------------------------------------\")\n",
    "for result in search_results: print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895f90df-2bea-4831-b5db-aa8d6eb43fef",
   "metadata": {},
   "source": [
    "### Retrieve Article Summary\n",
    "Now that we have a list of articles, we can request a summary of each article and display them. Note that if an article is ambiguous, the program will not attempt to retrieve the article. An ambiguous article is an article which could refer to multiple topics. The summary output from an ambiguous article will display possible searches that you may want to try. Since we are only interested in direct articles, the ambiguous titles will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99582e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# search\n",
    "search_results = wikipedia.search(\"Climate\")\n",
    "\n",
    "display_articles = []\n",
    "for i in range (0,len(search_results)):\n",
    "    try:\n",
    "        summary = wikipedia.summary(search_results[i])\n",
    "    except Exception as err:\n",
    "        print(f\"Skipped article '{search_results[i]}' skipped because of ambiguity.\")\n",
    "        continue\n",
    "        \n",
    "    display_articles.append({\n",
    "        \"title\"   : search_results[i],\n",
    "        \"summary\" : summary\n",
    "    })\n",
    "\n",
    "#print(display_articles)\n",
    "\n",
    "df = pd.DataFrame.from_dict(display_articles)\n",
    "df.style.set_properties(**{'text-align': 'left'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96792953",
   "metadata": {},
   "source": [
    "## Load a Wikipedia Article into watsonx.data \n",
    "This step will load selected articles into watsonx.data. Since we are only interested in climate change, we will select the first two articles in the list. You can change the documents loaded by changing the document indexes in the variable found in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b436a408-9066-42d1-8c2d-8748bd79ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45505e75-ce12-49af-8998-2e6a649b5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "# fetch wikipedia articles\n",
    "\n",
    "articles = {}\n",
    "for document in documents:\n",
    "    articles.update({display_articles[document][\"title\"] : None})\n",
    "\n",
    "for k,v in articles.items():\n",
    "    article = wikipedia.page(k)\n",
    "    articles[k] = article.content\n",
    "    print(f\"Successfully fetched article {k}\")\n",
    "\n",
    "print(f\"Successfully fetched {len(articles)} articles \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b70849-6f29-41dd-ad9d-2ea7acffd9bd",
   "metadata": {},
   "source": [
    "### Connect to watsonx.data\n",
    "The following code will use the Presto Magic commmands to load data in watsonx.data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f6cde-bd30-4987-b5f8-2c6ebda05c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run presto.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861981f3-f699-4009-9c7a-ec645899e776",
   "metadata": {},
   "source": [
    "The connection details should not change unless you are attempting to run this script from a Jupyter environment that is outside of the developer system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae4fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "   connect\n",
    "   userid=ibmlhadmin\n",
    "   password=password\n",
    "   hostname=watsonxdata\n",
    "   port=8443\n",
    "   catalog=tpch\n",
    "   schema=tiny\n",
    "   certfile=/certs/lh-ssl-ts.crt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723dc88-a7a3-4931-aa16-eb33e679a1be",
   "metadata": {},
   "source": [
    "## Create Schema in watsonx.data\n",
    "We need to create a new bucket to store the Wikipedia data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54ddf5-e999-4912-a1c2-652a6951ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS hive_data.watsonxai.wikipedia;\n",
    "DROP SCHEMA IF EXISTS hive_data.watsonxai;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b482bbd4-8988-4d5a-a3dc-196f5d9e4be5",
   "metadata": {},
   "source": [
    "The next step will delete any existing data in the watsonxai bucket. A DROP table command does not remove the files in the bucket. You may see error messages displayed if no data or bucket exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5107e0-d43a-4e93-a0c1-7c27c68ff6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "minio_host    = \"watsonxdata\"\n",
    "minio_port    = \"9000\"\n",
    "hive_host     = \"watsonxdata\"\n",
    "hive_port     = \"9083\"\n",
    "\n",
    "hive_id           = None\n",
    "hive_password     = None\n",
    "minio_access_key  = None\n",
    "minio_secret_key  = None\n",
    "keystore_password = None\n",
    "\n",
    "try:\n",
    "    with open('/certs/passwords') as fd:\n",
    "        certs = fd.readlines()\n",
    "    for line in certs:\n",
    "        args = line.split()\n",
    "        if (len(args) >= 3):\n",
    "            system   = args[0].strip()\n",
    "            user     = args[1].strip()\n",
    "            password = args[2].strip()\n",
    "            if (system == \"Minio\"):\n",
    "                minio_access_key = user\n",
    "                minio_secret_key = password\n",
    "            elif (system == \"Thrift\"):\n",
    "                hive_id = user\n",
    "                hive_password = password\n",
    "            elif (system == \"Keystore\"):\n",
    "                keystore_password = password\n",
    "            else:\n",
    "                pass\n",
    "except Error as e:\n",
    "    print(\"Certificate file with passwords could not be found\")\n",
    "\n",
    "%system mc alias set watsonxdata http://{minio_host}:{minio_port} {minio_access_key} {minio_secret_key}\n",
    "\n",
    "%system mc rm --recursive --force watsonxdata/hive-bucket/watsonxai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deb69f3-e0bb-4697-a017-026ffb30e6b0",
   "metadata": {},
   "source": [
    "### Create the Watsonxai Schema and Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eec943-b859-4c26-90fc-543be1fe1b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE SCHEMA IF NOT EXISTS \n",
    "  hive_data.watsonxai \n",
    "WITH ( location = 's3a://hive-bucket/watsonxai')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf1567f-6c6e-4845-8f51-dba6225c2986",
   "metadata": {},
   "source": [
    "### Create the Wikipedia Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f945d3f-4f94-4a18-84e0-491d19bc8fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE hive_data.watsonxai.wikipedia\n",
    "  (\n",
    "  \"id\" varchar,\n",
    "  \"text\" varchar,\n",
    "  \"title\" varchar  \n",
    "  )\n",
    "WITH \n",
    "  (\n",
    "  format = 'PARQUET',\n",
    "  external_location = 's3a://hive-bucket/watsonxai' \n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3c109b-5f1b-4f80-9525-0f4daa4755e8",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "The Wikipedia article is written into the watsonx.data database in chucks of approximately 225 words in size. The reason for chunking the data is to make it more efficient when populating the Milvus system from watsonx.data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a1c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk data\n",
    "def split_into_chunks(text, chunk_size):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "split_articles = {}\n",
    "for k,v in articles.items():\n",
    "    split_articles[k] = split_into_chunks(v, 225)\n",
    "\n",
    "# Insert data\n",
    "\n",
    "for article_title, article_chunks in split_articles.items():\n",
    "\n",
    "    for i, chunk in enumerate(article_chunks):\n",
    "            \n",
    "        escaped_chunk = chunk.replace(\"'\", \"''\").replace(\"%\", \"%%\")\n",
    "        insert_stmt = f\"insert into hive_data.watsonxai.wikipedia values ('{i+1}', '{escaped_chunk}', '{article_title}')\"\n",
    "        %sql --quiet {insert_stmt}\n",
    "        print(f\"{article_title} {i+1}/{len(article_chunks)} inserted\",end=\"\\r\")\n",
    "            \n",
    "    print(f\"\\n{article_title} Insertion complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a347b2-7206-4d1c-bd79-e2793a04ba8d",
   "metadata": {},
   "source": [
    "### Confirm that the Data has be Loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1cd529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "   select * from hive_data.watsonxai.wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41ac032-dabb-45ab-800b-3287e2d64f6d",
   "metadata": {},
   "source": [
    "## Load Vector Embeddings to Milvus\n",
    "\n",
    "Here we will take the data we loaded into watsonx.data from the previous step and load it into the vector database Milvus. This data was previously chunked and stored in a watsonx.data hive table, so we'll pull from here, vectorize the text chunks and load them into Milvus.\n",
    "\n",
    "Before we can start loading the data, though, we need to create a collection in Milvus to hold the data. We'll call this collection `wiki_articles`. This collection holds the vector embeddings for each chunk of text, as well as the original text itself and additional context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d65d50e-d9ff-41c6-bca3-5578fa4e3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f /tmp/presto.cert\n",
    "!echo QUIT | openssl s_client -showcerts -connect localhost:8443 | awk '/-----BEGIN CERTIFICATE-----/ {p=1}; p; /-----END CERTIFICATE-----/ {p=0}' > /tmp/presto.crt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae179cac-13b8-4856-91da-2fbaadabe7be",
   "metadata": {},
   "source": [
    "### Milvus Connection Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37cc34f-8962-4f94-b533-2b5719dd76f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "host            = 'watsonxdata'\n",
    "port            = 19530\n",
    "user            = 'ibmlhadmin'\n",
    "password        = 'password'\n",
    "server_pem_path = '/tmp/presto.crt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531096a8-d363-4715-aec1-b32ee38b4d1d",
   "metadata": {},
   "source": [
    "#### Generate a Connection to Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5954387-046a-4f3d-a41e-a9bff19c979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import(\n",
    "    Milvus,\n",
    "    IndexType,\n",
    "    Status,\n",
    "    connections,\n",
    "    FieldSchema,\n",
    "    DataType,\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    ")\n",
    "\n",
    "connections.connect(alias='default',\n",
    "                   host=host,\n",
    "                   port=port,\n",
    "                   user=user,\n",
    "                   password=password,\n",
    "                   server_pem_path=server_pem_path,\n",
    "                   server_name='watsonxdata',\n",
    "                   secure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6d49fa-8275-403c-962c-1738f4e17fcf",
   "metadata": {},
   "source": [
    "#### Create a Collection in Milvus\n",
    "This code will drop the wiki_articles collection if it exists, and then recreate it. This script should return the following text.\n",
    "```\n",
    "Status(code=0, message=)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf2243d-f81e-4324-971c-44dcf68b3a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import utility\n",
    "\n",
    "utility.drop_collection(\"wiki_articles\")\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True), # Primary key\n",
    "    FieldSchema(name=\"article_text\", dtype=DataType.VARCHAR, max_length=2500,),\n",
    "    FieldSchema(name=\"article_title\", dtype=DataType.VARCHAR, max_length=200,),\n",
    "    FieldSchema(name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=384),\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields, \"wikipedia article collection schema\")\n",
    "\n",
    "wiki_collection = Collection(\"wiki_articles\", schema)\n",
    "\n",
    "# Create index\n",
    "index_params = {\n",
    "        'metric_type':'L2',\n",
    "        'index_type':\"IVF_FLAT\",\n",
    "        'params':{\"nlist\":2048}\n",
    "}\n",
    "\n",
    "wiki_collection.create_index(field_name=\"vector\", index_params=index_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f67701f-24a5-4aac-9038-05b262a39696",
   "metadata": {},
   "source": [
    "#### Double Check that the Schema Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa147d48-dba2-4811-a9ba-47b6b1606690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import utility\n",
    "utility.list_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162fd740-8bc5-44ae-a5c5-8c7daba82298",
   "metadata": {},
   "source": [
    "### Insert Vectors into Milvus\n",
    "\n",
    "Here we read data from the watsonx.data table. We pull text chunks and titles from the database, being sure to separate them out into separate lists. We then vectorize using the `sentence-transformers/all-MiniLM-L6-v2` sentence transformer model. Learn more about Hugging Face sentence transformers here: [Sentence Transformers](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).\n",
    "\n",
    "It is important we assemble the article text, article titles and vector embeddings into a `data` object. This object will be used to load the data into Milvus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2b3cca-ca4a-4ca1-8497-5e8b22ef0949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pymilvus import Collection, connections\n",
    "import warnings\n",
    "import os\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download Wikipedia articles from watsonx.data using the engine we created earlier \n",
    "\n",
    "articles_df = %sql --pandas SELECT * from hive_data.watsonxai.wikipedia\n",
    "\n",
    "# extract text + titles\n",
    "\n",
    "passages = articles_df['text'].tolist()\n",
    "passage_titles = articles_df['title'].tolist()\n",
    "\n",
    "# Create vector embeddings + data\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2') # 384 dim\n",
    "passage_embeddings = model.encode(passages)\n",
    "\n",
    "basic_collection = Collection(\"wiki_articles\") \n",
    "data = [\n",
    "    passages,\n",
    "    passage_titles,\n",
    "    passage_embeddings\n",
    "]\n",
    "out = basic_collection.insert(data)\n",
    "basic_collection.flush()  # Ensures data persistence\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c331f52-7d9d-4608-a719-01196eeae486",
   "metadata": {},
   "source": [
    "#### Check that the Collection has been Loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99d084f-b8a1-4a88-b50e-5cd90f625035",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_collection = Collection(\"wiki_articles\") \n",
    "basic_collection.load()\n",
    "basic_collection.num_entities "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0830a9ea-4683-42be-8a83-a99629cab77e",
   "metadata": {},
   "source": [
    "## Query Milvus & Prompt LLM\n",
    "After gathering the data from Wikipedia and then vectorizing it and inserting into Milvus, we are now ready to perform queries against the vector database. We will use the `sentence-transformers/all-MiniLM-L6-v2` model to generate the query vector and then use Milvus to find the most similar vectors in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ffc8e8-383a-4798-bda5-8ddb95d86835",
   "metadata": {},
   "source": [
    "### Create a Query Function\n",
    "The following function will be used to query the Milvus database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96297151-2f1f-411e-885b-bd8da0d01d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pymilvus import(\n",
    "    Milvus,\n",
    "    IndexType,\n",
    "    Status,\n",
    "    connections,\n",
    "    FieldSchema,\n",
    "    DataType,\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    ")\n",
    "\n",
    "def query_milvus(query, num_results=5):\n",
    "    \n",
    "    # Vectorize query\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2') # 384 dim\n",
    "    query_embeddings = model.encode([query])\n",
    "\n",
    "    # Search\n",
    "    search_params = {\n",
    "        \"metric_type\": \"L2\", \n",
    "        \"params\": {\"nprobe\": 5}\n",
    "    }\n",
    "    results = basic_collection.search(\n",
    "        data=query_embeddings, \n",
    "        anns_field=\"vector\", \n",
    "        param=search_params,\n",
    "        limit=num_results,\n",
    "        expr=None, \n",
    "        output_fields=['article_text'],\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d035f2a5-da27-49fe-9822-efa9c75b568b",
   "metadata": {},
   "source": [
    "### Prompt LLM with Query Results\n",
    "Consider how climate change may relate to other industries and processes related to your business. Select one of the questions below to feed into Milvus query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336adfb9-9266-4378-9f6b-1d6dc62486eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_text = \"What can my company do to help fight climate change?\"\n",
    "#question_text = \"How do businesses negatively effect climate change?\"\n",
    "#question_text = \"What can a businesses do to have a positive effect on climate change?\"\n",
    "#question_text = \"How can a business reduce their carbon footprint?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9700c9b9-d27e-4eb8-a1bf-966e434a9be0",
   "metadata": {},
   "source": [
    "### Search a Question in Milvus\n",
    "An embedding is made for the question being asked. It is then used to search for the most relevant chunks in Milvus. The top 3 related chunks are retrieved below and can be used for a large language prompt.\n",
    "\n",
    "The documents that best match the question are found in the list below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541d200f-1cca-44fd-95e4-ace6c9c48e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "num_results = 3\n",
    "results = query_milvus(question_text, num_results)\n",
    "\n",
    "display_articles = []\n",
    "relevant_chunks  = []\n",
    "for i in range(num_results):\n",
    "    display_articles.append({\n",
    "        \"ID\"      : results[0].ids[i],\n",
    "        \"Distance\": results[0].distances[i],\n",
    "        # \"Article\" : re.sub(r\"^.*?\\. (.*$)\",r\"\\1\",results[0][i].entity.get('article_text'))\n",
    "        \"Article\" : re.sub(r\"^.*?\\. (.*\\.).*$\",r\"\\1\",results[0][i].entity.get('article_text'))        \n",
    "    })\n",
    "    relevant_chunks.append(re.sub(r\"^.*?\\. (.*\\.).*$\",r\"\\1\",results[0][i].entity.get('article_text')))\n",
    "\n",
    "df = pd.DataFrame.from_dict(display_articles).sort_values(\"Distance\",ascending=False)\n",
    "df.style.set_properties(**{'text-align': 'left'}).set_caption(question_text).set_table_styles([{\n",
    "    'selector': 'caption',\n",
    "    'props': [\n",
    "        ('color', 'blue'),\n",
    "        ('font-size', '20px')\n",
    "    ]\n",
    "}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c5d19-144d-4805-ae7c-5244c95044ce",
   "metadata": {},
   "source": [
    "### Generate a Prompt\n",
    "The data retrieved from Milvus can be used to generate a prompt for watsonx.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0237d259-7212-4952-badc-9c8dad95d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(context, question_text):\n",
    "    return (f\"{context}\\n\\nPlease answer a question using this text. \"\n",
    "          + f\"If the question is unanswerable, say \\\"unanswerable\\\".\"\n",
    "          + f\"\\n\\nQuestion: {question_text}\")\n",
    "\n",
    "\n",
    "# Build prompt w/ Milvus results\n",
    "# Embed retrieved passages(context) and user question into into prompt text\n",
    "\n",
    "context = \"\\n\\n\".join(relevant_chunks)\n",
    "prompt = make_prompt(context, question_text)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b26ac2c-c7e4-4f4e-8025-89fe9696810f",
   "metadata": {},
   "source": [
    "#### Credits: IBM 2025, Katherine Ciaravalli, George Baklarz [baklarz@ca.ibm.com]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
