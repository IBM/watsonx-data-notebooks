{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b09a288b-7761-44dd-9d4b-4aa9539664ee",
   "metadata": {},
   "source": [
    "![Top <](./images/watsonxdata.png \"watsonxdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183874ae-2185-4297-8b9f-216b45fb57a2",
   "metadata": {},
   "source": [
    "# Kafka Jupyter Notebook\n",
    "\n",
    "Apache Kafka® <sup style=\"color: blue;\">1,2</code></sup> is an event streaming platform. Kafka combines three capabilities so you can implement your use cases for event streaming.\n",
    "\n",
    "* To publish (write) and subscribe to (read) streams of events, including continuous import/export of your data from other systems.\n",
    "* To store streams of events durably and reliably for as long as you want.\n",
    "* To process streams of events as they occur or retrospectively.\n",
    "\n",
    "#### Producers and Consumers\n",
    "Producers are those client applications that publish (write) events to Kafka, and consumers are those that subscribe to (read and process) these events. In Kafka, producers and consumers are fully decoupled and agnostic of each other, which is a key design element to achieve the high scalability that Kafka is known for. For example, producers never need to wait for consumers. Kafka provides various guarantees such as the ability to process events exactly-once.\n",
    "\n",
    "#### Events\n",
    "Events are organized and durably stored in topics. Very simplified, a topic is similar to a folder in a filesystem, and the events are the files in that folder. An example topic name could be \"payments\". Topics in Kafka are always multi-producer and multi-subscriber: a topic can have zero, one, or many producers that write events to it, as well as zero, one, or many consumers that subscribe to these events. Events in a topic can be read as often as needed—unlike traditional messaging systems, events are not deleted after consumption. Instead, you define for how long Kafka should retain your events through a per-topic configuration setting, after which old events will be discarded. Kafka's performance is effectively constant with respect to data size, so storing data for a long time is perfectly fine.\n",
    "\n",
    "<p style=\"font-size: small;line-height: 0.4;\"><sup>1. Additional details can be found on the <a href=\"https://kafka.apache.org/documentation/#gettingStarted\">Kafka website</a>.</sup></p>\n",
    "<p style=\"font-size: small;line-height: 0.4;\"><sup>2. Thanks to Chunyu Jiang for prototyping the Kafka connector and suggestions on the code.</sup></p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57383907-b9e4-49a1-b16a-87a06b84ffe4",
   "metadata": {},
   "source": [
    "## Creating a Kafka Service\n",
    "\n",
    "The first step is to create a Kafka service in our system. The following command will create the control file required to create the Kafka Docker images. The system does not contain any topics when it starts up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377fc650-e59b-4585-bb84-58c3d2725633",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_compose = '''\n",
    "version: '3'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:latest\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "    ports:\n",
    "      - 22181:2181\n",
    "  \n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:latest\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    ports:\n",
    "      - 29092:29092\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://watsonxdata:29092\n",
    "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n",
    "      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "'''\n",
    "with open(\"kafka-compose.yaml\",\"w\") as fd:\n",
    "    fd.write(kafka_compose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f26855-0285-4b31-9de7-13e299f75796",
   "metadata": {},
   "source": [
    "Now we can start the Kafka and Zookeeper service. The default Kafka port is 29092. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b484c-461b-468f-92fe-09fd8986c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "%system docker compose -p kafka -f kafka-compose.yaml up --detach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89afbc70-b54e-44c0-b9fa-a9622bdf19c0",
   "metadata": {},
   "source": [
    "### Load Kafka Libraries\n",
    "Before we start using the system, we need to load the Python libraries required to communicate with Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946af311-8a88-45c4-a8da-42e86647cc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%system python3 -m pip install confluent-kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89be6c74-a38e-4630-9897-33739ccc5e9e",
   "metadata": {},
   "source": [
    "### Open up the Kafka Port\n",
    "We need to open the port that is used by Kafka so the client can communicate with the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10c4d8c-8d5a-45b6-8f13-081a694cfb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%system firewall-cmd --add-port=29092/tcp --zone=public --permanent\n",
    "%system firewall-cmd --reload"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e34f4dd4-559c-4e84-b35e-bdddb2b3cd3c",
   "metadata": {},
   "source": [
    "## Create a Topic\n",
    "\n",
    "Message queues are referred to as topics in the Kafka documentation. We need to create a topic that Kafka will use in our system. The following code will connect to the Kafka server and add the topic <code style=\"color:blue;background-color:transparent;\">watsonxdata</code>. The first step is to establish a connection to the Kafka service. If you receive an error message when running this code, it indicates that the Kafka service has not yet initialized. Try running the code again after a few seconds and then check to see if the topic has been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d0a9ee-d94b-4dc4-a07c-84dfccf0b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "\n",
    "admin_client = AdminClient(\n",
    "    {\n",
    "        'bootstrap.servers': 'watsonxdata:29092',\n",
    "    }\n",
    ")\n",
    "\n",
    "try:\n",
    "    res = admin_client.create_topics(new_topics=[\n",
    "            NewTopic(\n",
    "                topic='watsonxdata',\n",
    "                num_partitions=1,\n",
    "                replication_factor=1,\n",
    "            )\n",
    "        ],\n",
    "        validate_only=False,\n",
    "        operation_timeout=10,\n",
    "    )\n",
    "except Exception as err:\n",
    "    print(repr(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb07a53-4550-4a58-baf5-17d78cebf8d8",
   "metadata": {},
   "source": [
    "Check that our topic is there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3685cffd-6736-418e-83aa-507fc8b70588",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    topics = False\n",
    "    metadata = admin_client.list_topics()\n",
    "    for t in iter(metadata.topics.values()):\n",
    "        topics = True\n",
    "        print(f\"Topic: {t.topic}\")   \n",
    "except Exception as err:\n",
    "    print(repr(err))   \n",
    "\n",
    "if (topics == False):\n",
    "    print(\"No topics found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a813bca-a823-4562-b2d3-263be1e0eca9",
   "metadata": {},
   "source": [
    "## Registering the Kafka Service\n",
    "The next step is to register the Kafka service with watsonx.data. Navigate to your watsonx.data UI screen and click on the infrastructure icon.\n",
    "\n",
    "![Add Database](images/watsonx-kafka-add-database.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7855064-48c3-4ef1-8b76-bd7cb7ed90ce",
   "metadata": {},
   "source": [
    "Select the Add database option and select Apache Kafka as the source.\n",
    "\n",
    "![Add Database](images/watsonx-kafka-add-kafka.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27822e90-64ac-43e0-b1b6-1e4a7603af20",
   "metadata": {},
   "source": [
    "Enter the following settings into the dialog.\n",
    "\n",
    "* Display name - <code style=\"color:blue;background-color: transparent;\">kafka</code>\n",
    "* Hostname - <code style=\"color:blue;background-color: transparent;\">watsonxdata<code style=\"color:blue;background-color: transparent;\">\n",
    "* Port - <code style=\"color:blue;background-color: transparent;\">29092<code style=\"color:blue;background-color: transparent;\">\n",
    "* Topics - <code style=\"color:blue;background-color: transparent;\">watsonxdata<code style=\"color:blue;background-color: transparent;\">\n",
    "\n",
    "If your Kafka service had multiple topics, you can list them all in the Topics line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c966b7-5d98-4669-afc8-c1496e170d3a",
   "metadata": {},
   "source": [
    "![Add Database](images/watsonx-kafka-settings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f55bcc-49c5-42a0-ad79-c07c87b17655",
   "metadata": {},
   "source": [
    "Once you have entered these values into the panel, use the Test Connection link to make sure the Kafka server is reachable. Once you register the Kafka service, you should see it reflected in the infrastructure screen.\n",
    "\n",
    "![Add Database](images/watsonx-kafka-infrastructure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7590d2b3-3741-4487-aebf-9ec8105eaf58",
   "metadata": {},
   "source": [
    "At this point you must associate the Kafka service with the Presto engine. Hover over the Kafka catalog until you see the association icon. Click on that to view the associate dialog.\n",
    "\n",
    "![Add Database](images/watsonx-kafka-associate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cfafa5-2c4a-43d5-80de-cddfde143ab7",
   "metadata": {},
   "source": [
    "Select the presto-01 engine and press Save and restart engine. The Kafka service should now show as connected to the Presto engine.\n",
    "\n",
    "![Add Database](images/watsonx-kafka-associated.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b4a07-fae7-429e-bc05-80204a0254cd",
   "metadata": {},
   "source": [
    "It may take a few moments for the Presto engine to restart so any error codes you may receive in the following steps are due to the Presto service not being available yet.\n",
    "\n",
    "Switch to the Data Manager view.\n",
    "\n",
    "![Add Database](images/watsonx-kafka-datamanger-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4592cea7-a984-4dea-aaa5-0e1833900c8f",
   "metadata": {},
   "source": [
    "You should see the Kafka catalog in the list on the left. The schema is called <code style=\"color:blue;background-color: transparent;\">default</code>. If you don't see the service, you may need to refresh the screen to wait for Presto to become available. Clicking on the <code style=\"color:blue;background-color: transparent;\">watsonxdata</code> topic (which is mapped to a table) will display information about this queue.\n",
    "\n",
    "![Add Database](images/watsonx-kafka-datamanger-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff9af97-ca3a-4fc7-a2e9-25e96756a1a5",
   "metadata": {},
   "source": [
    "When data is loaded into the queue, you will be able to click on the Data sample tab to look at the messages that have come in.\n",
    "\n",
    "![Add Database](images/watsonx-kafka-data.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dbd468-b0c8-432e-9892-0cbfce6a855f",
   "metadata": {},
   "source": [
    "Note that the data will not be shown on your system yet since the messages have not yet been loaded into the queue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b8ee53-17a5-4597-9e26-a288e66643eb",
   "metadata": {},
   "source": [
    "## Producers and Consumers\n",
    "Kafka uses the concept of producers and consumers which was discussed at the beginning of this notebook. A producer will generate records that are sent to the Kafka topic (queue) and held for consumers to read. In this system, we are going to write a Python program that will read the customer.csv file (found in the staging-bucket directory) and place the records into the Kafka queue (watsonxdata) at a rate of 10 record per second. As these records are produced, we can view the queue in the watsonx.data UI or through SQL.\n",
    "\n",
    "The first step is to register a producer connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7523b90e-b71a-46a8-a286-5c7206882c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "\n",
    "producer = Producer(\n",
    "   {'bootstrap.servers': 'watsonxdata:29092'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb886781-9ad0-4bfa-a7da-e6a68b764757",
   "metadata": {},
   "source": [
    "### Read the CSV file into Pandas\n",
    "This code will import the CSV file and create a JSON record for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dd8bad-4cb9-4c45-9c51-e360661c880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_in = \"staging-bucket/customer.csv\"\n",
    "csv_in = \"staging-bucket/orders.csv\"\n",
    "with open(csv_in, newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    data = [row for row in reader]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf508d20-bec8-4f24-86c5-37b3f9925cfe",
   "metadata": {},
   "source": [
    "### Create the Workload Function\n",
    "When you run the next block of code, it will take each JSON record and send it to the Kafka queue. There is a check done to make sure that the <code style=\"color:blue;background-color: transparent;\">unit_price</code> value is a number and positive. There are some records in the data set that are invalid and this will remove them from the data. \n",
    "\n",
    "The function will first delete the contents of the Topic queue (watsonxdata) and then it will insert records per second into the queue. If you run the function in the foreground, there is no delay in the insert time. If the function is run in the background, it will insert 20 rows per second. \n",
    "\n",
    "Running the next cell will only register the function, not run it. You have two options for running this code:\n",
    "\n",
    "  * <code style=\"color:blue;background-color: transparent;\">sendMessages(True)</code> - The mode will run in the foreground, placing the records in the queue (No delay)\n",
    "  * <code style=\"color:blue;background-color: transparent;\">sendMessages(False)</code> - This is the default mode where the code will run in the background (10 messages/sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d0962f-beb3-406f-8166-80e1f6cae146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendMessages(inline=False):\n",
    "    \n",
    "    from time import sleep\n",
    "    from multiprocessing.pool import ThreadPool    \n",
    "    from confluent_kafka.admin import AdminClient, KafkaException\n",
    "    import json\n",
    "    import decimal\n",
    "\n",
    "    global running\n",
    "\n",
    "    def callback(message):\n",
    "        global running\n",
    "        running = False\n",
    "    \n",
    "    def insertMessages(inline):\n",
    "\n",
    "        if (inline == True):\n",
    "            pause = .01\n",
    "        else:\n",
    "            pause = .05        \n",
    "        \n",
    "        row_count = len(data)\n",
    "        current_row = 0\n",
    "        if (inline == True):\n",
    "            print(f\"Loading {row_count} rows into the queue\")\n",
    "        for row in data:\n",
    "            current_row += 1  \n",
    "            order_id    = row.get(\"order_id\",\"\").strip()\n",
    "            order_date  = row.get(\"order_date\",\"\").strip()\n",
    "            customer_id = row.get(\"customer_id\",\"\").strip()\n",
    "            product_id  = row.get(\"product_id\",\"\").strip()\n",
    "            quantity    = row.get(\"quantity\",\"\").strip()\n",
    "            unit_price  = row.get(\"unit_price\",\"\").strip()\n",
    "            if (order_id in [None,\"\"] or\n",
    "                order_date in [None,\"\"] or\n",
    "                customer_id in [None,\"\"] or\n",
    "                product_id in [None,\"\"] or\n",
    "                quantity in [None,\"\"] or\n",
    "                unit_price in [None,\"\"]):\n",
    "                if (inline == True): \n",
    "                    print(f\"\\nRow {current_row:4d} Skipped (invalid data)\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                uprice = decimal.Decimal(unit_price)\n",
    "                if (uprice <= 0):\n",
    "                    if (inline == True): \n",
    "                        print(f\"\\nRow {current_row:4d} Skipped (invalid price)\")\n",
    "                    continue\n",
    "            except:\n",
    "                if (inline == True): \n",
    "                    print(f\"\\nRow {current_row:4d} Skipped (invalid price)\")\n",
    "                continue\n",
    "\n",
    "            if (inline == True):\n",
    "                print(f\"Row {current_row:4d}/{row_count} Loaded                    \",end=\"\\r\")\n",
    "        \n",
    "            try:\n",
    "                producer.produce(\"watsonxdata\",value=json.dumps(row))\n",
    "                producer.flush()\n",
    "            except Exception as err:\n",
    "                return False\n",
    "            sleep(pause)\n",
    "\n",
    "        return False\n",
    "\n",
    "    running = True\n",
    " \n",
    "    if (inline == False):\n",
    "        pool = ThreadPool(processes=1)\n",
    "        async_result = pool.apply_async(insertMessages, (False,), callback=callback)    \n",
    "        print(\"Messages are being inserted in the background\")\n",
    "    else:\n",
    "        insertMessages(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e87a61-a42d-42e0-bfee-534472038dfd",
   "metadata": {},
   "source": [
    "## Connect to watsonx.data\n",
    "\n",
    "You can now connect to this Kafka queue through watsonx.data using a Presto connection. First load the Presto Magic commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04accaa4-ffcc-4e09-82a4-b8689573830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run presto.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de5b0b6-bab4-43cc-a7f5-02cd1842281a",
   "metadata": {},
   "source": [
    "We then connect to the watsonx.data engine with a connect command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488edee7-0402-4309-b398-6a18f9217cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "   connect\n",
    "   userid=ibmlhadmin\n",
    "   password=password\n",
    "   hostname=watsonxdata\n",
    "   port=8443\n",
    "   catalog=tpch\n",
    "   schema=tiny\n",
    "   certfile=/certs/lh-ssl-ts.crt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0042ee15-f357-486d-bc04-d18b0b58a197",
   "metadata": {},
   "source": [
    "### Delete the Contents of a Topic\n",
    "The data is available through the <code style=\"color:blue;background-color: transparent;\">kafka</code> catalog. The schema is called <code style=\"color:blue;background-color: transparent;\">default</code> when it is added to the watsonx.data system. The table name will be the name of the topic (queue). The next cell will make sure that there is no data in the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff33ad-b33f-4cce-a233-ce66d22cc996",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_client = AdminClient(\n",
    "    {\n",
    "        'bootstrap.servers': 'watsonxdata:29092',\n",
    "    }\n",
    ")    \n",
    "\n",
    "try:\n",
    "    metadata = admin_client.delete_topics(['watsonxdata'])\n",
    "    for topic, f in metadata.items():\n",
    "        print(f\"Topic {topic} deleted\")\n",
    "    \n",
    "except Exception as err:\n",
    "    print(repr(err))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399bff9-51f8-4d9b-b222-de9616fc10bc",
   "metadata": {},
   "source": [
    "The Kafka server has been configured to auto-create Topics. What this means is that when a Topic is deleted (and the messages discarded), the Topic no longer exists, but will be recreated when a producer or consumer sends a request to use that topic. Trying to run the above command will result in an error message after a Topic has been deleted. However, the Topic does exist in the Kafka catalog.\n",
    "\n",
    "You can check that the topic does not exist using the next command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f838eaa7-472a-4a40-83de-dcd54e9277d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    topics = False\n",
    "    metadata = admin_client.list_topics()\n",
    "    for t in iter(metadata.topics.values()):\n",
    "        topics = True\n",
    "        print(f\"Topic: {t.topic}\")   \n",
    "except Exception as err:\n",
    "    print(repr(err))   \n",
    "\n",
    "if (topics == False):\n",
    "    print(\"No topics found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173fd0c9-9565-4904-af9d-1100f63a6469",
   "metadata": {},
   "source": [
    "We can also check using the watsonx.data connection to get a count of messages in the Topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d527346-934f-4bd4-9910-cc5d47aa738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select count(*) from  \"kafka\".\"default\".\"watsonxdata\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa11d74-7659-4595-88b8-0c483cbcd74d",
   "metadata": {},
   "source": [
    "Note how the topic has now been recreated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c3f2e-b1a7-481a-a908-636a4984d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    topics = False\n",
    "    metadata = admin_client.list_topics()\n",
    "    for t in iter(metadata.topics.values()):\n",
    "        topics = True\n",
    "        print(f\"Topic: {t.topic}\")   \n",
    "except Exception as err:\n",
    "    print(repr(err))   \n",
    "\n",
    "if (topics == False):\n",
    "    print(\"No topics found\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004a8c5f-fc49-4d76-b533-6fc41ac2ddb7",
   "metadata": {},
   "source": [
    "### Start the Workload\n",
    "The next statement will begin inserting messages into the queue. If you want to load all messages immediately, change the <code style=\"color:blue;background-color: transparent;\">run_foreground</code> parameter from <code style=\"color:blue;background-color: transparent;\">False</code> to <code style=\"color:blue;background-color: transparent;\">True</code>. Approximately 500 rows will be placed into the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c6682f-a7e3-462d-9ee9-cc95cc5fc518",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_foreground = False\n",
    "sendMessages(run_foreground)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd1f3e-c0ec-470c-b6e4-b481fe916822",
   "metadata": {},
   "source": [
    "If the program is running in the background, we can check the progress by using SQL. The sendMessages code (above) will set a Python variable (running) to False when it stops creating messages.\n",
    "\n",
    "The next block of code will run until the insert process is complete. The code checks the record count every second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad29b9-8a34-465d-b79b-5d48ffd2a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "while (running == True):\n",
    "    x = %sql --raw select count(*) from \"kafka\".\"default\".\"watsonxdata\"\n",
    "    print(f\"SQL count  : {x[0][0]:4}\",end=\"\\r\")\n",
    "    sleep(1)\n",
    "\n",
    "x = %sql --raw select count(*) from \"kafka\".\"default\".\"watsonxdata\"\n",
    "print(f\"\\nFinal count: {x[0][0]:4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d4cd6-a2de-473a-bc73-a41304207e35",
   "metadata": {},
   "source": [
    "## Kafka Consumer\n",
    "Once the messages are on the queue, you can read them by using a Kafka Consumer. This block of code will connect to the <code style=\"color:blue;background-color: transparent;\">watsonxdata</code> Topic and read the records on the queue. The returned JSON records will be shredded and placed into a Pandas dataframe for viewing.\n",
    "\n",
    "The <code style=\"color:blue;background-color: transparent;\">group.id</code> in the program below is generated each time the code is run. This guarantees that the program will see all of the messages since the topic was created (offest: earliest). If you place a value in there (i.e., watsonx), the program will read the data on the first run. After that, it will only read messages that are \"new\". The code will exit after 10 attempts at reading something from the topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfdfa8b-33b1-4dc4-a945-3253745f4854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer\n",
    "from IPython.display import display, HTML\n",
    "import uuid\n",
    "\n",
    "consumer = Consumer(\n",
    "   {\n",
    "    'bootstrap.servers'   : 'watsonxdata:29092',\n",
    "    'group.id'            : uuid.uuid1(),\n",
    "    'session.timeout.ms'  : 6000,\n",
    "    'default.topic.config': {\n",
    "        'auto.offset.reset': 'earliest'\n",
    "    } \n",
    "   }\n",
    ")\n",
    "\n",
    "consumer.subscribe([\"watsonxdata\"])\n",
    "\n",
    "rows = []\n",
    "\n",
    "retries = 0\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(1)\n",
    "        if msg is None:\n",
    "            retries += 1\n",
    "            if (retries <= 10 and len(rows) == 0):\n",
    "                pass\n",
    "            else:\n",
    "                break\n",
    "        elif msg.error():\n",
    "            print(\"Error: %s\".format(msg.error()))\n",
    "            break\n",
    "        else:\n",
    "            retries = 0\n",
    "            row = msg.value().decode('utf8')\n",
    "            rows.append(row)\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    consumer.close()\n",
    "\n",
    "if (len(rows) > 0):\n",
    "    json_array = [ json.loads(row) for row in rows ]\n",
    "    df = pd.DataFrame(json_array)\n",
    "    display(HTML(df.to_html()))\n",
    "else:\n",
    "    print(\"No records found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0594998-6e44-4e7b-af27-9b4d0836282a",
   "metadata": {},
   "source": [
    "## Watsonx.data SQL\n",
    "You can view the data in watsonx.data using the SQL interface in the UI. You can also use Presto SQL to view the data. The following example will shred the message contents into column using the built-in JSON functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc1c794-1325-4779-8e29-cf298a9858f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "  cast(json_extract_scalar(_message,'order_id') as integer) as order_id,\n",
    "  cast(json_extract_scalar(_message,'order_date') as date) as order_date,\n",
    "  cast(json_extract_scalar(_message,'customer_id') as integer) as customer_id,\n",
    "  cast(json_extract_scalar(_message,'product_id') as integer) as product_id,\n",
    "  cast(json_extract_scalar(_message,'quantity') as integer)  as quantity,\n",
    "  cast(json_extract_scalar(_message,'unit_price') as decimal(7,2)) as unit_price\n",
    "FROM\n",
    "  \"kafka\".\"default\".\"watsonxdata\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb75c4e3-7510-43d2-a2f9-acdbc7b94ce3",
   "metadata": {},
   "source": [
    "### Clear the Messages\n",
    "We can clear the messages from the topic (Queue) and attempt to query the data again to recreate the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab0ea9-740b-473a-84aa-8234827dd119",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_client = AdminClient(\n",
    "    {\n",
    "        'bootstrap.servers': 'watsonxdata:29092',\n",
    "    }\n",
    ")    \n",
    "try:\n",
    "    res = admin_client.delete_topics(['watsonxdata'])\n",
    "except Exception as err:\n",
    "    print(repr(err))\n",
    "\n",
    "%sql select count(*) from \"kafka\".\"default\".\"watsonxdata\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a5f218-2451-4364-aca0-f0fcd0e70bb9",
   "metadata": {},
   "source": [
    "## Shutdown Kafka\n",
    "The following code will shutdown our Kafka service. This will remove all of the Topics that were created so that the scripts in this section can be run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d688e-b8f7-4460-ae3d-f1eaba088c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%system docker compose -p kafka -f kafka-compose.yaml down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d6baf-1c96-49b0-bff4-8fb222689142",
   "metadata": {},
   "source": [
    "#### Credits: IBM 2024, George Baklarz [baklarz@ca.ibm.com], Chunyu Jiang [Chunyu.Jiang@ibm.com]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
